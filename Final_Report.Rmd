---
title: "Final Report"
author: "Will Gullion, Brian Nalley, Nate Nellis, McKade Thomas"
date: "STAT 5650"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, include=FALSE}
library(dplyr)
library(MASS)
library(verification)
library(caTools)
library(klaR)
library(randomForest)
library(glmnet)
library(ggplot2)
library(knitr)
library(gridExtra)
```

```{r data, include=FALSE}
health <- read.csv("heart_disease_health_indicators.csv")

health <- health %>% mutate(
  HeartDiseaseorAttack = as.factor(HeartDiseaseorAttack),
  HighBP = as.factor(HighBP),  # LASSO approved
  HighChol = as.factor(HighChol),  # LASSO approved
  CholCheck = as.factor(CholCheck),
  ln.BMI = log(BMI),  # RF approved
  Smoker = as.factor(Smoker),  # LASSO approved
  Stroke = as.factor(Stroke),  # LASSO 1-SE only, RF approved
  Diabetes = as.factor(Diabetes),  # LASSO approved, RF approved
  PhysActivity = as.factor(PhysActivity),
  Fruits = as.factor(Fruits),
  Veggies = as.factor(Veggies),
  HvyAlcoholConsump = as.factor(HvyAlcoholConsump),
  AnyHealthcare = as.factor(AnyHealthcare),
  NoDocbcCost = as.factor(NoDocbcCost),  # LASSO approved
  GenHlth = as.factor(GenHlth),  # LASSO approved, RF approved
  ln.MentHlth = log(MentHlth + 1), # Probably shouldn't use
  bin.MentHlth = as.factor(ifelse(MentHlth == 0, 0, 1)),  #RF approved
  fac.MentHlth = as.factor(MentHlth), # Probably shouldn't use
  ln.PhysHlth = log(PhysHlth + 1), # Probably shouldn't use
  bin.PhysHlth = as.factor(ifelse(PhysHlth == 0, 0, 1)),  # LASSO approved, RF approved
  fac.PhysHlth = as.factor(PhysHlth), # Probably shouldn't use
  DiffWalk = as.factor(DiffWalk),  # LASSO , RF approved
  Sex = as.factor(ifelse(Sex == 0, "F", "M")),  # LASSO approved, RF approved
  Age = as.factor(Age),  # LASSO approved, RF approved
  Education = as.factor(Education),  # RF approved
  Income = as.factor(Income)  # LASSO approved, RF approved
)


best_features <- c(1:4, 6:15, 18:23, 25, 28)
health_subset <- health[,best_features]


# Train and Test Sets
health2_split <- sample.split(health_subset, SplitRatio = 0.3)
health_test <- subset(health_subset, health2_split == TRUE)
health_train <- subset(health_subset, health2_split == FALSE)

```

# Introduction

We chose to focus on data having to do with heart disease/heart attach indicators to understand which factors impact a person's probability to have a heart attach or summer from strong levels of heart disease. Data was found on Kaggle and was modified from a CDC survey.  See the _Data Dictionary_ section for more details at the end of the document. The dataset contains 252,680 observations. The response variable we have is a binary one telling us if the person being interviewed suffered from a heart attack or not. There are a total of 20 available predictor variables. 

# Exploratory Data Analysis

Taking a closer look at our predictor variable, we noticed that only 9.4% of participants in the survey had heart disease or a heart attack, 23,893 responded yes versus 229,787 who responded no.  This means that we could just predict a 0 for all observations and we could get an accuracy of over 90%. Since we aren't concerned in this instance with a high accuracy, but actually a decent sensitivity (getting accurate results when someone is at risk of a heart attack or heart disease), we utilized Upsampling in some of our methods to skew the model to predict the results we wanted.  This decreased overall accuracy, but resulted in a much better prediction accuracy of finding those people with heart disease or who had a heart attack.  Even with a loss in accuracy, we felt that this was a wise move as predicting someone who has heart disease is much more important than identifying someone who doesn't have heart disease. See the below bar chart to compare the two catagories visually.

```{r, echo=FALSE, fig.height=3}
ggplot(health, aes(HeartDiseaseorAttack)) +
  geom_bar() +
  scale_y_continuous(limits = c(0, 250000), breaks = seq(0, 250000, by = 50000)) +
  ylab("Count") +
  ggtitle("Count of HeartDiseaseorAttack Observations - Before Upsampling") +
  theme(panel.grid.major = element_line(colour = "black"),
        panel.background = element_rect(fill = NA))
```

When it comes to the predictor variables there are 12 binary, 3 numerical, and 5 factor variables. Having fewer numerical variables is less than desireable, especially when it comes to how some of the factor variables are coded/binned.  An example of this terrible survey coding is the __Income__ variable where the CDC survey sets the highest bucket capped at `$`75,000+.  Due to how low that is for the maximum response allowed, a full third of the observations fall into that category.

```{r, echo=FALSE}
ggplot(health, aes(Income)) +
  geom_bar() +
  scale_y_continuous(limits = c(0, 100000), breaks = seq(0, 100000, by = 25000)) +
  ylab("Count") +
  ggtitle("Count of Income Observations") +
  theme(panel.grid.major = element_line(colour = "black"),
        panel.background = element_rect(fill = NA))
```

For the three numerical variables, __BMI__, __MentHlth__, and __PhysHlth__, there is a definite skewness that we can see in their distributions.  This is especially true for __PhysHlth__ and __MentHlth__, which are how many times in the last 30 days a survey respondent reported having poor physical or mental health respectively.  A huge percentage of respondents reported zero days, 70% for __MentHlth__ and 63% for __PhysHlth__.  To correct for these skewed variables, we logged __BMI__ to enforce normality constraints.  The For __MentHlth__ and __PhysHlth__ we created binary variables where any value greater than 0 was coded as a 1 and was coded as a 0 otherwise. We originally attempted to log these variables but the skew wasn't reduced very much and we ended up finding that the binary variables worked out much better. These transformations were valuable in the various classification methods that we tried, even though they did reduce our numerical variables to really just the __ln.BMI__ variable.

```{r, echo=FALSE, fig.height=4}
grid.arrange(
ggplot(health, aes(BMI)) +
  geom_histogram(bins=30) +
  scale_y_continuous(limits = c(0, 75000), breaks = seq(0, 75000, by = 25000)) +
  ylab("Count") +
  ggtitle("Histogram of BMI") +
  theme(panel.grid.major = element_line(colour = "black"),
        panel.background = element_rect(fill = NA))
,
ggplot(health, aes(ln.BMI)) +
  geom_histogram(bins=30) +
  scale_y_continuous(limits = c(0, 75000), breaks = seq(0, 75000, by = 25000)) +
  ylab("Count") +
  ggtitle("Histogram of Logged BMI") +
  theme(panel.grid.major = element_line(colour = "black"),
        panel.background = element_rect(fill = NA))
,
nrow = 1)
grid.arrange(
ggplot(health, aes(MentHlth)) +
  geom_histogram(bins=30) +
  scale_y_continuous(limits = c(0, 200000), breaks = seq(0, 200000, by = 50000)) +
  ylab("Count") +
  ggtitle("Histogram of MentHlth") +
  theme(panel.grid.major = element_line(colour = "black"),
        panel.background = element_rect(fill = NA))
,
ggplot(health, aes(PhysHlth)) +
  geom_histogram(bins=30) +
  scale_y_continuous(limits = c(0, 200000), breaks = seq(0, 200000, by = 50000)) +
  ylab("Count") +
  ggtitle("Histogram of PhysHlth") +
  theme(panel.grid.major = element_line(colour = "black"),
        panel.background = element_rect(fill = NA))
,
nrow = 1)
```

See more information on the variables in the __Data Dictionary__ Section at the end of the document.
















# Upsample and feature subsets from RF and LASSO

```{r}
## UpSample to deal with class imbalance
new_train <- upSample(train[,2:22], as.factor(train[,1]), list=FALSE) %>%
  mutate(HeartDiseaseorAttack = as.numeric(Class)-1)

# new_train$HeartDiseaseorAttack <- new_train$Class
# new_train <- new_train[,-22]

## Test Set Upsample ##
new_test <- upSample(test[,2:22], as.factor(test[,1]), list = FALSE) %>%
  mutate(HeartDiseaseorAttack = as.numeric(Class)-1)
# new_test$HeartDiseaseorAttack <- new_test$Class
# new_test <- new_test[,-22]

rf_best_features <- new_train %>% dplyr::select(HeartDiseaseorAttack, Age, GenHlth, ln.BMI, Income, HighBP, Education, HighChol)

rf_test_features <- new_test %>% dplyr::select(HeartDiseaseorAttack, Age, GenHlth, ln.BMI, Income, HighBP, Education, HighChol)

lasso_best_features <- new_train %>% dplyr::select(HeartDiseaseorAttack, HighBP,
                                                   HighChol, CholCheck, Smoker,
                                                   Stroke, Diabetes, HvyAlcoholConsump,
                                                   NoDocbcCost, GenHlth, DiffWalk,
                                                   Sex, Age, Income, bin.PhysHlth)

lasso_test_features <- new_test %>% dplyr::select(HeartDiseaseorAttack, HighBP,
                                                   HighChol, CholCheck, Smoker,
                                                   Stroke, Diabetes, HvyAlcoholConsump,
                                                   NoDocbcCost, GenHlth, DiffWalk,
                                                   Sex, Age, Income, bin.PhysHlth)
```

# Random Forests

```{r}
set.seed(89729)
train_sample <- sample_n(rf_best_features, 10000)
test_sample <- sample_n(rf_test_features, 10000)

rf <- randomForest(as.factor(HeartDiseaseorAttack) ~ ., data = train_sample, n.trees = 100, importance = TRUE)

class.sum(train_sample$HeartDiseaseorAttack, predict(rf, type="prob")[,2], cap = "Training Data RF Performance")
class.sum(test_sample$HeartDiseaseorAttack, predict(rf, test_sample, type="prob")[,2], cap = "Test Data RF Performance")

```

We built the random forest using the predictors that the variable importance plot indicated had the largest effects on the random forest's classification ability. Those variables ended up being Age, GenHlth, ln.BMI, Income, HighBP, Education, and HighChol. After creating the random forest, we found that it classified approximately 75% of the training set correctly. The next step was to validate the random forest using the testing set. We saw similar performance, with just under 75% correctly classified. Perhaps more important than the PCC was the sensitivity and specificity which came in at 79% and 70%, respectively. We see a huge increase in the sensitivity, thanks to our upsampling technique. 





















# Data Dictionary  
Data found by us at https://www.kaggle.com/datasets/alexteboul/heart-disease-health-indicators-dataset  

Description of variables found in https://www.cdc.gov/brfss/annual_data/2015/pdf/codebook15_llcp.pdf  

## Response Variable  
  
__HeartDiseaseorAttack__, binary  

```{r}
summary(health$HeartDiseaseorAttack)
```
  
## Predictor Variables  
  
__HighBP__, binary  
 - Adults who have been told they have high blood pressure by a doctor, nurse, or other health professional  
```{r}
summary(health$HighBP)
```
__HighChol__, binary  
- Have you EVER been told by a doctor, nurse or other health professional that your blood cholesterol is high?  
```{r}
summary(health$HighChol)
```
__CholCheck__, binary  
- Cholesterol check within past five years  
```{r}
summary(health$CholCheck)
```
__BMI__, numerical  
```{r}
summary(health$BMI)
hist(as.numeric(health$BMI))
```
__ln.BMI__, numerical  
- Logged version of the BMI variable to reduce the impact of outliers and enforce better normality contstraints.  
```{r}
summary(health$ln.BMI)
hist(as.numeric(health$ln.BMI))
```
__Smoker__, binary  
- Have you smoked at least 100 cigarettes in your entire life?  
```{r}
summary(health$Smoker)
```
__Stroke__, binary  
```{r}
summary(health$Stroke)
```
__Diabetes__, factor  
- 0 is no diabetes,  
- 1 is pre-diabetes  
- 2 is diabetes  
```{r}
summary(health$Diabetes)
```
__PhysActivity__, binary  
- During the past month, other than your regular job, did you participate in any physical activities or exercises such as running, calisthenics, golf, gardening, or walking for exercise?  
```{r}
summary(health$PhysActivity)
```
__Fruits__, binary  
- Consume Fruit 1 or more times per day  
```{r}
summary(health$Fruits)
```
__Veggies__, binary  
- Consume Vegetables 1 or more times per day  
```{r}
summary(health$Veggies)
```
__HvyAlcoholConsum__, binary  
- Heavy drinkers (adult men having more than 14 drinks per week and adult women having more than 7 drinks per week)  
```{r}
summary(health$HvyAlcoholConsum)
```
__AnyHealthcare__, binary  
- Do you have any kind of health care coverage, including health insurance, prepaid plans such as HMOs, or government plans such as Medicare, or Indian Health Service?  
```{r}
summary(health$AnyHealthcare)
```
__NoDocbcCost__, binary  
- Was there a time in the past 12 months when you needed to see a doctor but could not because of cost?  
```{r}
summary(health$NoDocbcCost)
```
__GenHlth__, factor  
- Would you say that in general your health is:  
- 1 = Excellent  
- 2 = Very Good  
- 3 = Good  
- 4 = Fair  
- 5 = Poor  
```{r}
summary(health$GenHlth)
```
__MentHlth__, numerical  
- Now thinking about your mental health, which includes stress, depression, and problems with emotions, for how many days during the past 30 days was your mental health not good?  
```{r}
summary(health$MentHlth)
```
__bin.MentHlth__, binary  
- Binary version of __MentHlth__ where 0 days is coded as 0 and any number of days is coded as 1.
```{r}
summary(health$bin.MentHlth)
```
__PhysHlth__, numerical  
- Now thinking about your physical health, which includes physical illness and injury, for how many days during the past 30 days was your physical health not good?   
```{r}
summary(health$PhysHlth)
```
__bin.PhysHlth__, binary  
- Binary version of __PhysHlth__ where 0 days is coded as 0 and any number of days is coded as 1.  
```{r}
summary(health$bin.PhysHlth)
```
__DiffWalk__, binary  
- Do you have serious difficulty walking or climbing stairs?  
```{r}
summary(health$DiffWalk)
```
__Sex__, binary  
- 0 = Female  
- 1 = Male  
```{r}
summary(health$Sex)
```
__Age__, factor  
- 1 Age 18 to 24  
- 2 Age 25 to 29  
- 3 Age 30 to 34  
- 4 Age 35 to 39  
- 5 Age 40 to 44  
- 6 Age 45 to 49  
- 7 Age 50 to 54  
- 8 Age 55 to 59  
- 9 Age 60 to 64  
- 10 Age 65 to 69  
- 11 Age 70 to 74  
- 12 Age 75 to 79  
- 13 Age 80 or older  
- 14 Don't Know / Refused to answer (I removed these as well)  
```{r}
summary(health$Age)
hist(as.numeric(health$Age))
```
__Education__, factor  
- What is the highest grade or year of school you completed?  
- 1 Never attended school or only kindergarten  
- 2 Grades 1 through 8 (Elementary)  
- 3 Grades 9 through 11 (Some high school)  
- 4 Grade 12 or GED (High school graduate)  
- 5 College 1 year to 3 years (Some college or technical school)  
- 6 College 4 years or more (College graduate)  
```{r}
summary(health$Education)
hist(as.numeric(health$Education))
```
__Income__, factor  
- Is your annual household income from all sources  
- 1 Less than `$`10,000  
- 2 Less than `$`15,000 (`$`10,000 to less than `$`15,000)  
- 3 Less than `$`20,000 (`$`15,000 to less than `$`20,000)  
- 4 Less than `$`25,000 (`$`20,000 to less than `$`25,000)  
- 5 Less than `$`35,000 (`$`25,000 to less than `$`35,000)  
- 6 Less than `$`50,000 (`$`35,000 to less than `$`50,000)  
- 7 Less than `$`75,000 (`$`50,000 to less than `$`75,000)  
- 8 `$`75,000 or more  
```{r}
summary(health$Income)
hist(as.numeric(health$Income))
```